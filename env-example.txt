# ======================================
# RAG Chatbot Backend - Environment Variables
# ======================================
# Copy this file to .env and fill in your actual values

# ======================================
# REQUIRED VARIABLES
# ======================================

# PostgreSQL database connection string
# Format: postgresql://username:password@host:port/database
DATABASE_URL="postgresql://username:password@localhost:5432/rag_chatbot"

# Google Gemini API key (required for Gemini provider)
# Get from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY="your-gemini-api-key-here"

# ======================================
# DATABASE CONFIGURATION
# ======================================

# ChromaDB vector database host
CHROMA_HOST="localhost"

# ChromaDB vector database port
CHROMA_PORT="8000"

# ======================================
# PROCESSING CONFIGURATION
# ======================================

# Maximum file size in MB for uploads
MAX_FILE_SIZE_MB="10"

# Minimum similarity score for answers (0.0 to 1.0)
RELEVANCE_THRESHOLD="0.75"

# LLM temperature (maximum 0.3 for factual responses)
LLM_TEMPERATURE="0.3"

# Maximum tokens in LLM responses
LLM_MAX_TOKENS="1024"

# Embedding vector dimensions
EMBEDDING_DIMENSIONS="768"

# ======================================
# MODEL PROVIDER CONFIGURATION
# ======================================
# Default: Google Gemini (recommended for production)

# LLM provider: "gemini" or "local"
LLM_PROVIDER="gemini"

# LLM model name (for Gemini provider)
LLM_MODEL_NAME="gemini-pro"

# Embedding provider: "gemini" or "local"
EMBEDDING_PROVIDER="gemini"

# Embedding model name (for Gemini provider)
EMBEDDING_MODEL_NAME="embedding-001"

# ======================================
# LOCAL MODEL CONFIGURATION (Optional)
# ======================================
# Uncomment and configure these if using local LLM providers
# Examples: Ollama, LM Studio, or custom endpoints

# Local LLM API endpoint (when LLM_PROVIDER=local)
# Example for Ollama: http://localhost:11434/v1/chat/completions
# LLm_ENDPOINT="http://localhost:11434/v1/chat/completions"

# Local embedding API endpoint (when EMBEDDING_PROVIDER=local)
# Example for Ollama: http://localhost:11434/v1/embeddings
# EMBEDDING_ENDPOINT="http://localhost:11434/v1/embeddings"

# ======================================
# DEVELOPMENT/DEBUGGING (Optional)
# ======================================

# Enable debug logging (set to * for all logs)
# DEBUG="rag-chatbot:*"

# ======================================
# PRODUCTION NOTES
# ======================================
# - Never commit .env files with real API keys
# - Use strong, unique passwords for database
# - Consider using environment-specific .env files (.env.production, .env.staging)
# - For production deployments, use secure key management services